{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSI4106_RL_Fall20.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsCQnroFWbpA"
      },
      "source": [
        "# Notebook 10 - Reinforcement Learning / Self-Driving Cab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS9f1M7hWblx"
      },
      "source": [
        "CSI4106 Artificial Intelligence   \n",
        "Fall 2020  \n",
        "Prepared by Julian Templeton and Caroline Barri√®re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWknU1E2Wbi8"
      },
      "source": [
        "***INTRODUCTION***:  \n",
        "In this notebook we will be exploring the use of Reinforcment Learning to help allow an agent solve a specific task in an environment provided by [OpenAI's Gym library](https://gym.openai.com/). This library provides a number of environments that we can train an AI to master. Within this notebook we will be exploring a scenario in which a taxi located on a grid must be controlled by an agent to pickup a passenger located in one of four positions and drop the passenger off in one of three other positions.    \n",
        "\n",
        "To familiarize yourself with the Self-Driving Cab problem tackled in this notebook, please go to the site https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/ and read section 1 (rewards), section 2 (state space) which will make you understand why there are 500 possible states, section 3 (action space) which describes the possible actions.  \n",
        "\n",
        "Throughout the notebook we will be working with a Baseline approach and a Q-Learning-based approach. This will provide insight into how Q-Learning can be applied to problems and how an agent can use Reinforcment Learning to solve problems in an environment.    \n",
        "\n",
        "**When submitting this notebook, ensure that you do NOT reset the outputs from running the code (plus remember to save the notebook with ctrl+s).**      \n",
        "\n",
        "**In order to keep the installation easy, you will be once again running this notebook in Google Colab, NOT on your local machine.**    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iCMGO9VWbgM"
      },
      "source": [
        "***HOMEWORK***:  \n",
        "Go through the notebook by running each cell, one at a time.  \n",
        "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, Sign the notebook (at the end of the notebook), and submit it.  \n",
        "\n",
        "*The notebook will be marked on 30.  \n",
        "Each **(TO DO)** has a number of points associated with it.*\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIhGNTB-Wbbs"
      },
      "source": [
        "**1.0 - Setting up the Taxi Game**   \n",
        "\n",
        "To begin the notebook, we will need to set up and explore the environment that our agent will be working with. OpenAI's Gym provides many different experiments to use. These range from balancing acts to self driving cars to playing a simple Atari game. Unfortunately, not every option available to us can be easily worked with. Many can take hours of training to start seeing some exciting results. Each of these experiments use agents that can be trained by Reinforcment Learning to master how to perform the specified task. The methods used can range from the simple use of Q-Learning to the more complex use of one or more Deep Learning models that work in conjunction with Reinforcement Learning techniques.   \n",
        "\n",
        "One simple, yet interesting, experiment involves an AI controlled taxi that must pick up and dropoff a passenger. This is the problem that we will be exploring throughout the notebook. The code used throughout the notebook comes from [this example](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/) and has been modified accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEQK-Xep0MrF"
      },
      "source": [
        "To start, we will install some of the packages that we will need to run the progam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPYIG4d6Ztzk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6ddf02-7e17-40de-f2fc-55906e3cdc87"
      },
      "source": [
        "# Install the necessary libraries\n",
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0vOVuqHA8SM"
      },
      "source": [
        "# Import the necessary libraries\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3nhPCUHb2cO"
      },
      "source": [
        "With all of the libraries installed, we will now make use of the Taxi program provided by Gym. Below we will import Gym, load the program as the active environment, and render an image representing the current state of the program.   \n",
        "\n",
        "From the image seen below, there are four different key locations in the environment, represented by *R*, *G*, *B*, and *Y*. The letter with that is bolded in blue represents where the current passenger needs to get picked up and the letter bolded in purple represents where the passenger wants to dropped off. The yellow block represents the cell which the taxi cab is currently located at. Therefore, the taxi cab must first pick up the passenger and drop them off at the dropoff location. When a passenger is in the taxi, it turns green until the passenger is dropped off."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2iMY2MbaFSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33518830-cd47-4b2d-a3df-298c0dc34608"
      },
      "source": [
        "# Load the environment\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "# Render the current state of the program\n",
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oq2BvrwdPh-"
      },
      "source": [
        "Next we will reset the state of the environment and re-render the current state. We also print the total number of actions available to our agent (defined as the *Action Space*) and the *State Space* which represents the state of the program (where is the cab, the passenger, pickup location and dropoff location)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlLevJ7KaHYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77761d3b-c827-4546-9dd5-00496fb7fca3"
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGp8quCqeGjU"
      },
      "source": [
        "Intuitively, we want our agent to learn which action to take given a specific state. Specifically, which action should be taken based on where the taxi cab is located in relation to the passenger location and drop off location. The six possible actions that the taxi can take at a given time step are:    \n",
        "\n",
        "Action = 0: Head south    \n",
        "Action = 1: Head north    \n",
        "Action = 2: Head east    \n",
        "Action = 3: Head west    \n",
        "Action = 4: Pickup     \n",
        "Action = 5: Dropoff    \n",
        "\n",
        "Below is an example of setting the state to a specific encoding and rendering that state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7QM71s0aded",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1152eb-896f-4def-a9d1-062a79ec8116"
      },
      "source": [
        "# The encoding below represents: (taxi row, taxi column, passenger index, destination index)\n",
        "state = env.encode(3, 1, 2, 0) \n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCSHJMfb2Hll"
      },
      "source": [
        "The following example showcases how a state with the passenger within the taxi can be set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPshGWDomKfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b556e4db-23b1-4ab8-c50c-b1e42ad04487"
      },
      "source": [
        "# The encoding below represents: (taxi row, taxi column, passenger index, destination index)\n",
        "state = env.encode(0, 1, 4, 0) \n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 36\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m:\u001b[42m_\u001b[0m| : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3b__QWRTRB3"
      },
      "source": [
        "**(TO DO) Q1**    \n",
        "Now that we have seen how to set a state via an encoding, you will need to set the state to match the descriptions below and render them.   \n",
        "a) Set the passenger to be at position G, with the passenger wanting to be dropped off at position R, and the taxi positioned at a random point on the grid (the selected position of the taxi must be selected randomly). After setting the position, render the state.          \n",
        "b) Set the passenger to be in the taxi (at any position without a letter on it) and set the passenger dropoff point to be position B. After setting the position, render the state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKbEIP2WAkb"
      },
      "source": [
        "**(TO DO) Q1 (a) - 2 marks**    \n",
        "a) Set the passenger to be at position G, with the passenger wanting to be dropped off at position R, and the taxi positioned at a random point on the grid (the selected position of the taxi must be selected randomly). After setting the position, render the state.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT05ErEOTRPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21588199-2606-43e1-e83f-f85bf83cb35a"
      },
      "source": [
        "# TODO (remember to use random coordinates within the grid for the taxi)...\n",
        "taxi_row = random.randint(0,4)\n",
        "taxi_col = random.randint(0,4)\n",
        "pass_ind = 1\n",
        "dest_ind = 0\n",
        "\n",
        "\n",
        "state = env.encode(taxi_row, taxi_col, pass_ind, dest_ind) \n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 264\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pl8bVtwWIdE"
      },
      "source": [
        "**(TO DO) Q1 (b) - 2 marks**    \n",
        "b) Set the passenger to be in the taxi (at any position without a letter on it) and set the passenger dropoff point to be position B. After setting the position, render the state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6roy4E4NWIv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a4f90a-57af-469d-8162-5ec4d2afe618"
      },
      "source": [
        "# TODO ...\n",
        "\n",
        "taxi_row = random.randint(1,3)\n",
        "taxi_col = random.randint(1,2)\n",
        "\n",
        "dest_ind = 3\n",
        "\n",
        "\n",
        "state = env.encode(taxi_row, taxi_col, 4, dest_ind) \n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 139\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| :\u001b[42m_\u001b[0m| : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTyOS_Hbi3Rf"
      },
      "source": [
        "For every action that the taxi can take, we have a list representing the key information with respect to what will happen when an action is performed. After performing an action, the agent will receive a reward or a penalty. This reward or penalty will tell the agent how good or bad their decision to perform the specified action was.     \n",
        "\n",
        "Below we display a dictionary that contains all possible actions along with the following information within the corresponding tuples:     \n",
        "\n",
        "(     \n",
        "  The probability of taking that action,     \n",
        "  The resulting state after taking that action,    \n",
        "  The reward for taking that action,    \n",
        "  Whether or not the program will end when performing the action   \n",
        ")      \n",
        "\n",
        "Example tuple: (1.0, 328, -1, False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6Ga3Y6yagfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd7d5da2-996e-44c2-f705-9b3ca0b2e378"
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_57Z2adUzAJH"
      },
      "source": [
        "Although not displayed by the code above, if the taxi is holding the passenger and is over the dropoff point, the reward for the dropoff action is 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iiy-KNusvBDJ"
      },
      "source": [
        "**2.0 - Baseline Approach to the Taxi Game**   \n",
        "\n",
        "To start, we will perform the simulation of the taxi cab scenario with a baseline approach that does not use Q-Learning. This approach will simply work by selecting a random available action at each time step, regardless of the current state. We will also prepare a method of playing through all frames within an episode to view how the agent controls the taxi in the scenario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKnHAA5SaitM"
      },
      "source": [
        "def run_single_simulation_baseline(env, state, disable_prints=False):\n",
        "    '''\n",
        "    Given the environment and a specific state, randomly select an action for the taxi\n",
        "    to perform until the goal is completed.\n",
        "    '''\n",
        "    if not disable_prints:\n",
        "        print(\"Testing for simulation: {}\".format(state))\n",
        "    # Set the state of the environment\n",
        "    env.s = state\n",
        "    # Used to hold all information for a single time step (including the image data)\n",
        "    frames = []\n",
        "    # Used to determine when the simulation has been completed\n",
        "    done = False\n",
        "    # Determines the number of times steps that the application has been run for\n",
        "    time_steps = 0\n",
        "    # The total values used to determine how many times the agent mistakenly\n",
        "    # picks up no one or attempts to dropoff no passenger or attempts to\n",
        "    # dropoff a passenger in the wrong position.\n",
        "    penalties, reward = 0, 0\n",
        "    # Run until the passenger has been picked up and dropped off in the target location\n",
        "    while not done:\n",
        "        # Perform a random action from the set of available actions in the environment\n",
        "        action = env.action_space.sample()\n",
        "        # From performing the action, retrieve the new state, the reward from taking the action,\n",
        "        # whether the simulation is complete, and other information from performing the action.\n",
        "        state, reward, done, info = env.step(action)\n",
        "        # If an incorrect dropoff or pickup is performed, increment the penalty count\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        # Put each rendered frame into dict to use for animating the process and\n",
        "        # tracking the details over the run\n",
        "        frames.append({\n",
        "            'frame': env.render(mode='ansi'),\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward\n",
        "            }\n",
        "        )\n",
        "        # Increment the time step count\n",
        "        time_steps += 1\n",
        "    # State the total number of steps taken and the total penalties that have occured.\n",
        "    if not disable_prints:\n",
        "        print(\"Timesteps taken: {}\".format(time_steps))\n",
        "        print(\"Penalties incurred: {}\".format(penalties))\n",
        "    # Return the frame data, the total penalties, and the total time steps\n",
        "    return frames, penalties, time_steps"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk5lkc3Q5XLv"
      },
      "source": [
        "With the baseline approach defined, we will run a test with this approach to see how long it takes an agent using this approach to find a solution for simulation 328 and how many major penalties the agent receives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTLKSCRq4bEG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36aa9088-21b0-4f67-90cb-b059169f1929"
      },
      "source": [
        "state = 328\n",
        "# Run a test and collect all frames from the run\n",
        "frames, _, _ = run_single_simulation_baseline(env, state)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing for simulation: 328\n",
            "Timesteps taken: 354\n",
            "Penalties incurred: 108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayN1up479Fl1"
      },
      "source": [
        "After performing a simulation and retrieving the results, we can use the frames obtained from the simulation and pass it to the *print_frames* function below to display an animation containing all frames along with the information that was from each time step that the frame corresponds to.    \n",
        "\n",
        "For the first episode that you view, it is recommended to run through the entire process at a slower speed (such as 0.3 or 0.5 in the sleep call). However you are free to increase the speed of the process by reducing the number in the sleep function call in the *print_frames* function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt-sMubValJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf11ae1f-9d6d-4d69-e6c4-c8417a119104"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    '''\n",
        "    For each frame, show the frame and display the timestep it occurred at,\n",
        "    the number of the active state, the action selected, adn the corresponding reward.\n",
        "    '''\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        # Can adjust speed here\n",
        "        sleep(.01)\n",
        "# Print the frames from the episode\n",
        "print_frames(frames)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 354\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRCoIPbi7OCv"
      },
      "source": [
        "**(TO DO) Q2**   \n",
        "a) Using the state defined from Q1 (a), retrieve the corresponding frames obtained from using the baseline approach above. Then display those frames.     \n",
        "b) Using the state defined from Q1 (b), retrieve the corresponding frames obtained from using the baseline approach above. Then display those frames.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-N9fP_Ydvot"
      },
      "source": [
        "**(TO DO) Q2 (a) - 2 marks**   \n",
        "a) Using the state defined from Q1 (a), retrieve the corresponding frames obtained from using the baseline approach above. Then display those frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDCSqmbM-USp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fa99593-811c-4b87-bc0e-b270e2ea151e"
      },
      "source": [
        "# TODO: Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (a). Then show those frames.\n",
        "state = 264\n",
        "# Run a test and collect all frames from the run\n",
        "frames, _, _ = run_single_simulation_baseline(env, state)\n",
        "print_frames(frames)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 1609\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Dufixud28j"
      },
      "source": [
        "**(TO DO) Q2 (b) - 2 marks**   \n",
        "b) Using the state defined from Q1 (b), retrieve the corresponding frames obtained from using the baseline approach above. Then display those frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMcngIsad3Mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c130122-6c64-4b66-d090-cace802fcc63"
      },
      "source": [
        "# TODO: Retrieve the corresponding frames from running the simulation starting from the state found in Q1 (b). Then show those frames.\n",
        "state = 139\n",
        "# Run a test and collect all frames from the run\n",
        "frames, _, _ = run_single_simulation_baseline(env, state)\n",
        "print_frames(frames)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 997\n",
            "State: 475\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwgxcH_Ojfud"
      },
      "source": [
        "With the ability to simulate single runs of an episode with the Baseline approach, we will now define a function that we will use to evaluate the general performance of the baseline model when averaged over many episodes. The *evaluate_agent_baseline* function below accepts as input the total number of randomly selected episodes to run along with the environment, runs the random episodes, displays the average amount of timesteps taken per episode along with the average penalties incurred, and returns the frame data.     \n",
        "\n",
        "***If the evaluate_agent_baseline function ever seems to be running for far too long (several minutes, not just one), stop the run by clicking the button at the top-left of the code cell being executed and run it again.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GLAT5_mjgEu"
      },
      "source": [
        "def evaluate_agent_baseline(episodes, env):\n",
        "    '''\n",
        "    Given a number of episodes and an environment, run the specified\n",
        "    number of episodes, where each run begins with a random state, display the\n",
        "    naverage timesteps per episode and the average penalties per episode, and output\n",
        "    the frames to be displayed.\n",
        "    '''\n",
        "    total_time_steps, total_penalties = 0, 0\n",
        "    frames = []\n",
        "    # Run through the total number of episodes\n",
        "    for _ in range(episodes):\n",
        "        # Get a random state\n",
        "        state = env.reset()\n",
        "        # Run the simulation, obtaining the results\n",
        "        frame_data, penalties, time_steps = run_single_simulation_baseline(env, state, True)\n",
        "        # Update the tracked data over all simulations\n",
        "        total_penalties += penalties\n",
        "        total_time_steps += time_steps\n",
        "        frames = frames + frame_data\n",
        "    print(f\"Results after {episodes} episodes:\")\n",
        "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
        "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "    return frames"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHkepDd2C3im"
      },
      "source": [
        "**(TO DO) Q3**    \n",
        "a) Use the *evaluate_agent_baseline* function defined above to run through 100 random episodes for the environment.     \n",
        "b) From the output seen from Q3 (a), how did the Baseline approach do and why do you think that it performed well or poorly? Explain with respect to the average timesteps per episode and the average penalties per episode.       \n",
        "c) Without moving to a Reinforcment Learning approach how can the Baseline approach be modified to perform slightly better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3Tnw6bBxMXv"
      },
      "source": [
        "**(TO DO) Q3 (a) - 1 mark**    \n",
        "a) Use the *evaluate_agent_baseline* function defined above to run through 100 random episodes for the environment.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPRemQq-xXUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cefaa670-95c0-4093-d234-53633511f56e"
      },
      "source": [
        "# TODO ...\n",
        "f = evaluate_agent_baseline(100, env)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 2231.29\n",
            "Average penalties per episode: 721.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dClJX6AWxZM5"
      },
      "source": [
        "**(TO DO) Q3 (b) - 1 mark**    \n",
        "b) From the output seen from Q3 (a), how did the Baseline approach do and why do you think that it performed well or poorly? Explain with respect to the average timesteps per episode and the average penalties per episode.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxUTqzCSxhZU"
      },
      "source": [
        "TODO ...    \n",
        "\n",
        "The Baseline approach performed very poorly. For example, previous tests, for the same configuration of the 'game', the baseline approach was capable of having way lower penalties (in the 100s) than the one being returned from th eevaluation of the approach (721.97). The same applies to the timesteps as well. The random aspect of the baseline approach is the reason behind the poor results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wr3F6IExrQQ"
      },
      "source": [
        "**(TO DO) Q3 (c) - 1 mark**    \n",
        "c) Without moving to a Reinforcment Learning approach how can the Baseline approach be modified to perform slightly better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_b7h54SDQ7C"
      },
      "source": [
        "TODO ...  \n",
        "We could hardcode the code in a way that we can:   \n",
        "*   avoid dropping people off we haven't reached the destination point\n",
        "*   avoid picking up people from the wrong point \n",
        "*   avoid dropping people off when there is nobody in the cab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GfrTVl3AuhD"
      },
      "source": [
        "**3.0 - Training an Agent with Q-Learning to play the Taxi Game**   \n",
        "\n",
        "Now that we have had an agent use the baseline model to complete the taxi simulation, we will have the agent use Q-Learning to try applying a Reinforcement Learning approach to the problem. To start the process, we will create a matrix of Q values for each action-state possibility (initializing it as zero). The agent will update this matrix when training and will need the matrix reset whenever the agent wants to reset its training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDTbzsaEanoU"
      },
      "source": [
        "# Initialize the table of Q values for the state-action pairs\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mph6_GdSGy2F"
      },
      "source": [
        "With the matrix of Q values initialized, we will now define the training function that adjusts the Q values within *q_table*. The training process consists of running through a number of random simulations and updating the Q values for each state via Q-Learning.    \n",
        "\n",
        "There are a number of hyperparameters used by the training function:    \n",
        "\n",
        "- *alpha*: Learning parameter (you will need to describe it in a later question).   \n",
        "- *gamma*: The long term reward discount parameter.    \n",
        "- *epsilon*: Exploitation/Exploration parameter (you will need to describe it in a later question).  \n",
        "- *num_simulations*: Represents how many random episodes should be generated to have the agents use to update its Q values.    \n",
        "\n",
        "Thus, by running through this algorithm, an agent can learn which Q-values to use when working with other episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kbLhT-Ua0Cc"
      },
      "source": [
        "def train_agent(alpha, gamma, epsilon, num_simulations):\n",
        "    '''\n",
        "    Trains an agent by updating its Q values for a total of num_simulations\n",
        "    episodes with the alpha, gamma, and epsilon hyperparameters. \n",
        "    '''\n",
        "    # For plotting metrics\n",
        "    all_time_steps = []\n",
        "    all_penalties = []\n",
        "    # Generate the specified number of episodes\n",
        "    for i in range(1, num_simulations + 1):\n",
        "        # Generate a new state by resetting it\n",
        "        state = env.reset()\n",
        "        # Variables tracked (time steps, total penalties, the reward value)\n",
        "        time_steps, penalties, reward, = 0, 0, 0\n",
        "        done = False\n",
        "        # Run the simulation \n",
        "        while not done:\n",
        "            # Select a random action is the randomly selected number from a\n",
        "            # uniform distribution is less than epsilon\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action = env.action_space.sample() # Explore action space\n",
        "            # Otherwise use the currently learned Q values\n",
        "            else:\n",
        "                action = np.argmax(q_table[state]) # Exploit learned values\n",
        "            # Retrieve the relevant information after performing the action\n",
        "            next_state, reward, done, info = env.step(action) \n",
        "            # Retrieve the old Q value and the maximum Q value from the next state\n",
        "            old_value = q_table[state, action]\n",
        "            next_max = np.max(q_table[next_state])\n",
        "            # Update the current Q value\n",
        "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "            q_table[state, action] = new_value\n",
        "            # Track anytime an incorrect dropoff or pickup is made\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "            # Proceed to the next state and time step\n",
        "            state = next_state\n",
        "            time_steps += 1\n",
        "        # Display progress for each 100 episodes\n",
        "        if i % 100 == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\"Episode: {i}\")\n",
        "    print(\"Training finished.\\n\")"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLoNggkqQu2L"
      },
      "source": [
        "We now use the training function with a set of hyperparameters to train the agent with Q-Learning to potentially improve performance over time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tkJvaTwPjfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a972dc7-f60b-4cf1-f29a-44cf45952394"
      },
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.5\n",
        "epsilon = 0.1\n",
        "num_simulations = 100000\n",
        "# Train the agent\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmzQceEvRyT3"
      },
      "source": [
        "After the training, we can look at the Q-values that have been obtained in our state-action table for a specific state. Below we see that each Q-value for the six possible actions available for state 328 have been updated accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXOzHtVAar6"
      },
      "source": [
        "**(TO DO) Q4 - 2 marks**     \n",
        "Below we print the Q-values that are available for the six actions at state 328 and we render that state to view it. Based on the available Q-values (assuming we are in exploitation mode), which action would be the next to be selected (or if there are ties, list all possible actions that would be considered)? Do any of the actions that contain larger Q values seem problematic if they were selected? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KP5FMBQa1el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9affe33a-5bc7-4a4b-e4a9-7bc64eb0ebdd"
      },
      "source": [
        "print(q_table[328])\n",
        "env.s = 328\n",
        "env.render()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ -1.9864243   -1.95703125  -1.98470273  -1.97617815 -10.74944318\n",
            " -10.49604289]\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx4axJNOBl_0"
      },
      "source": [
        "TODO ... \n",
        "The action that would be next to be selected is action with index 1: North \n",
        "Actions with index 4 & 5 do have larger negative Q values as they represent dropping off and picking up passengers and in state 328 both actions are not appropriate and would therefore add a large immediate negative reward.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncwGTV5PSHZx"
      },
      "source": [
        "With the training complete, we can now evaluate the Q-Learning approach in a similar method that we used to evaluate the Baseline approach. By passing the number of episodes to test for and the environment, we generate that number of random episodes and average the results obtained from running the Q-Learning approach to complete the episodes. Unlike the training, it is important to note that the hyperparameters that are used there are not used here. The agent simply uses the maximum Q-value at each step to determine which action to take at a given time step.   \n",
        "\n",
        "***If the evaluate_agent_QL function ever seems to be running for far too long (a minute or more), stop the run by clicking the button at the top-left of the code cell being executed and run it again. This occurs because the training was insufficient at setting valid Q values and resulted in a dead-end for a specific state.***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuitaI8Ra2u8"
      },
      "source": [
        "def evaluate_agent_QL(episodes, env):\n",
        "    '''\n",
        "    Given a number to specify how many random states to run and the environment to use,\n",
        "    display the averaged metrics obtained from the tests and return the frames obtained from the tests.\n",
        "    '''\n",
        "    total_time_steps, total_penalties = 0, 0\n",
        "    frames = []\n",
        "    for _ in range(episodes):\n",
        "        # Generate a random state to use\n",
        "        state = env.reset()\n",
        "        # The information collected throughout the run\n",
        "        time_steps, penalties, reward = 0, 0, 0\n",
        "        # Determines when the episode is complete\n",
        "        done = False\n",
        "        # Run through the episode until complete\n",
        "        while not done:\n",
        "            # Select the action containing the maximum Q value\n",
        "            action = np.argmax(q_table[state])\n",
        "            # Run that action and retrieve the reward and other details\n",
        "            state, reward, done, info = env.step(action)\n",
        "\n",
        "            # Put each rendered frame into dict for animation\n",
        "            frames.append({\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "                }\n",
        "            )\n",
        "            # Specify whether the agent incorrectly chose to pick up or dropoff a passenger\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "            # Increment the current time step\n",
        "            time_steps += 1\n",
        "        # Track the totals\n",
        "        total_penalties += penalties\n",
        "        total_time_steps += time_steps\n",
        "    # Display the performance over the tests\n",
        "    print(f\"Results after {episodes} episodes:\")\n",
        "    print(f\"Average timesteps per episode: {total_time_steps / episodes}\")\n",
        "    print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "    # Return the frames to allow a user to view the runs\n",
        "    return frames"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEQNpWoSF822"
      },
      "source": [
        "**(TO DO) Q5**     \n",
        "a) Run the *evaluate_agent_QL* for 100 episodes to retrieve the average number of time steps and the average penalty after training.     \n",
        "b) Given your results from Q5 (a), how do the observed results from the tests compare to the tests from the Baseline model in Q3 (a)? Specifically, which agent performs better with respect to the average number of penalties throughout the tests and which agent is able to solve the problems quicker (on average).    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TzyRekwGolK"
      },
      "source": [
        "**(TO DO) Q5 (a) - 1 mark**     \n",
        "a) Run the *evaluate_agent_QL* for 100 episodes to retrieve the average number of time steps and the average penalty after training.     "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAliG8sjTzBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "addb12f6-d44d-4ed3-bef7-c90613d0f724"
      },
      "source": [
        "# TODO ...\n",
        "ql = evaluate_agent_QL(100, env)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.78\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_Os_s3UGxK2"
      },
      "source": [
        "**(TO DO) Q5 (b) - 2 marks**     \n",
        "b) Given your results from Q5 (a), how do the observed results from the tests compare to the tests from the Baseline model in Q3 (a)? Specifically, which agent performs better with respect to the average number of penalties throughout the tests and which agent is able to solve the problems quicker (on average).     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPjrUlM4G_f_"
      },
      "source": [
        "TODO ... \n",
        "Overall, the results from the tests from Q5 shows that the new model performs a lot better than the Baseline model in Q3. With respect to the average number of penalties, the Baseline model have an average penalty greater than 0 whereas the new model has no penalties in any of the 100 episodes as the its average was, at least for the tests performed, equal to 0. In addition to that, the baseline model took a lot more steps to complete an episode than the ones taken by the new model.    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woAhj7vqaD5q"
      },
      "source": [
        "**4.0 - Testing Different Hyperparameters**   \n",
        "\n",
        "Now we will try retraining the agent using different set ups for the hyperparameters. This will allow you to explore their impact on the Q-Learning as well as understand their purpose during the training.     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBamDNUIZ0Iy"
      },
      "source": [
        "**(TO DO) Q6**     \n",
        "Below we explore variations for all four hyperparameters used by the Q-Learning approach to better understand their impact on the training. When answering the questions, ***be careful to correctly set the hyperparameters***.   \n",
        "\n",
        "a) Retrain the agent by resetting the Q learning values and training for only **35000 episodes** (with the same alpha, gamma, and epsilon values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.     \n",
        "b) Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **epsilon value of 0.8** (with the same alpha and gamma values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.    \n",
        "c) Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **alpha value of 0.7** (with the same gamma and epsilon values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.    \n",
        "d) Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **gamma value of 0.15** (with the same alpha and epsilon values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.    \n",
        "e) Based on your knowledge describe what the alpha and epsilon values are within the training function (i.e. what do they affect/do).    \n",
        "f) Using the results obtained from your tests in Q6 (a), (b), (c), and (d), along with the initial results found from Q5, explain the impacts of modifying the number of episodes trained on (less vs more), the alpha value (lower vs higher), the gamma value (lower vs higher), and the epsilon value (lower vs higher). Even if the difference in the comparisons are minor, state them.       \n",
        "\n",
        "As a note, below are the initial hyperparameter values used from section 3.0 of this notebook to use as reference:    \n",
        "\n",
        "*alpha* = 0.1   \n",
        "*gamma* = 0.5   \n",
        "*epsilon* = 0.1   \n",
        "*num_simulations* = 100000 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQxxvn0xaxrC"
      },
      "source": [
        "**(TO DO) Q6 (a) - 2 marks**     \n",
        "a) Retrain the agent by resetting the Q learning values and training for only **35000 episodes** (with the same alpha, gamma, and epsilon values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxQbWXNha4EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87482b74-5d71-409d-a60e-05cdfa48e0c7"
      },
      "source": [
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.5\n",
        "epsilon = 0.1\n",
        "num_simulations = 35000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 100 episodes\n",
        "q6a = evaluate_agent_QL(100, env)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 35000\n",
            "Training finished.\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.07\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kSCmM6VFdHu"
      },
      "source": [
        "**(TO DO) Q6 (b) - 2 marks**      \n",
        "b) Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **epsilon value of 0.8** (with the same alpha and gamma values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0vyZ7TNFeF7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb103b3a-3cf7-474e-abfd-cec0b0ec42bf"
      },
      "source": [
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.5\n",
        "epsilon = 0.8\n",
        "num_simulations = 100000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 100 episodes\n",
        "q6b = evaluate_agent_QL(100, env)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.86\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWcn_doxFm4H"
      },
      "source": [
        "**(TO DO) Q6 (c) - 2 marks**      \n",
        "c) Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **alpha value of 0.7** (with the same gamma and epsilon values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSM0Ert2FnHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18930921-4ce1-436c-f9d4-e9727369e9a1"
      },
      "source": [
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.7\n",
        "gamma = 0.5\n",
        "epsilon = 0.1\n",
        "num_simulations = 100000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 100 episodes\n",
        "q6c = evaluate_agent_QL(100, env)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.05\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZwH8XogFreB"
      },
      "source": [
        "**(TO DO) Q6 (d) - 2 marks**      \n",
        "d) Retrain the agent by resetting the Q learning values and training for **100000 episodes**, but with an **gamma value of 0.15** (with the same alpha and epsilon values used in section 3.0 of this notebook). Then perform another test for 100 episodes with the environment.    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVFGLvcZFrDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae81fa2f-a84c-401d-e726-6c756ab0a094"
      },
      "source": [
        "# TODO: Reset q_table\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "# TODO: Retrain with the specified hyperparameters\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.15\n",
        "epsilon = 0.1\n",
        "num_simulations = 100000\n",
        "train_agent(alpha, gamma, epsilon, num_simulations)\n",
        "# TODO: Test for 100 episodes\n",
        "q6d = evaluate_agent_QL(100, env)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 13.05\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY-Vgmn8GQ_9"
      },
      "source": [
        "**(TO DO) Q6 (e) - 2 marks**      \n",
        "e) Based on your knowledge describe what the alpha and epsilon values are within the training function (i.e. what do they affect/do).    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ro9AnLRhGRtV"
      },
      "source": [
        "TODO ...  \n",
        "*   Alpha is the learning factor which affects in which proportions the addition of the old Q-value and the new Q-value result in the new Q-value\n",
        "*   At every state, the model would have epsilon chances of taking a random action (exploring) rather than an action that would be recommend by the strategy (exploting)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ec4qnmyJbWYw"
      },
      "source": [
        "**(TO DO) Q6 (f) - 4 marks**      \n",
        "f) Using the results obtained from your tests in Q6 (a), (b), (c), and (d), along with the initial results found from Q5 to serve as the Q-Learning baseline to compare with, explain the impacts of modifying the number of episodes trained on (less vs more), the alpha value (lower vs higher), the gamma value (lower vs higher), and the epsilon value (lower vs higher). Even if the difference in the comparisons are minor, state them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3mYBjObbMi"
      },
      "source": [
        "TODO ... \n",
        "\n",
        "\n",
        "*   Less simulations during the training of the model increases the number of steps to reach the goal. \n",
        "*   A higher epsilon values did increase the amount of time taken to train the model maybe because random actions were more frequent \n",
        "*   A higher alpha value increased the number of steps taken to reach the goal during the 100 episodes used to test the model\n",
        "*   A lower gamma value increase the number of steps taken to reach the goal during the 100 episodes used to test the model \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiD_vGE6jMOu"
      },
      "source": [
        "***SIGNATURE:***\n",
        "My name is Ange Michaella Niyonkuru.\n",
        "My student number is 8962161.\n",
        "I certify being the author of this assignment."
      ]
    }
  ]
}