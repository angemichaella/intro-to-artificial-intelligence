{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CkFQVsf7hBV"
   },
   "source": [
    "# Notebook 9 - GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5PCrI7a7hBW"
   },
   "source": [
    "CSI4106 Artificial Intelligence   \n",
    "Fall 2020  \n",
    "Prepared by Julian Templeton and Caroline Barri√®re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edd5V5Zy7hBW"
   },
   "source": [
    "***INTRODUCTION (Read this carefully!)***:  \n",
    "\n",
    "With the ressurgance of Deep Learning and Deep Learning Architectures (DLAs) due to the increased power of modern computing, there are a multitude of ways in which DLAs can be used to tackle essentially every type of issue (from face recognition, to stock market prediction, to image generation, ...).    \n",
    "\n",
    "The generation of fake images, videos, music, and text (ex: poems) have been a topic of interest in society. These are issues with practical applications, such as in movies, but can have major ethical implications as well. That said, these DLAs are very interesting to work with and can be implemented easily through the use of prominent Python Deep Learning libraries such as [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [Keras](https://keras.io/). That said, although it is possible to find many examples of how these DLAs can be used, they still require immense computational power to provide effective results.     \n",
    "\n",
    "In this notebook we will be exploring a common image generation task through the use of a Generative Adversarial Learner (GAN). Starting with randomly generated images like this    \n",
    "\n",
    "![RandomNoise.png](data:image/png;base64,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)     \n",
    "\n",
    "the GAN will learn to generate images of digits like this (in this image, this digit is '1')  \n",
    "\n",
    "![ExampleDigit.png](data:image/png;base64,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)\n",
    "\n",
    "There are many types of GANs, each with different pros and cons. In this notebook we will be working with a DCGAN (Deep Convolutional GAN). A DCGAN is similar to a GAN, but introduces convolutional layers into its network to increase performance over the GAN's simple, fully connected network. Convolutional layers are commonly used in Convolutional Neural Networks, thus the DCGAN will showcase how these can be translated into the design of a GAN's Generator and Discriminator. To review these concepts you can look at the optional video in Module 6 on Brightspace to learn about convolutional layers in Convolutional Neural Networks (CNNs).        \n",
    "\n",
    "As alluded to above, DLAs such as GANs require immense computational processing. This typically requires a high- to top-end graphics card such as the latest cards from NVIDIA (which provide tools, such as [CUDA](https://developer.nvidia.com/cuda-zone) to allow code to be executed on the GPU for fast processing). Since we cannot expect you to have a graphics card or to spend the time waiting for a CPU  to perform the computations, we will be using a free cloud-based Jupyter environment that is provided by Google. [Google Colab](https://colab.research.google.com/) is a free, cloud-based jupyter environment that is great to perform basic Deep Learning experiments on since it provides access to some very powerful graphics cards (that have limits, but these should not occur for the duration of this notebook). These limits include time constraints and memory limitations that may occur if we run a training algorithm for too many epoch or if we design a model that has too many hidden nodes. Thus, you will be using this environment for the notebook and must carefully follow the instructions to ensure that you are working correctly within the new environment. Unlike previous notebooks, this notebook will focus on the exploration of ideas and the analysis of results since it would be beyond the scope of the course to learn a complex new library and program a complex Deep Learning model with it.    \n",
    "\n",
    "This notebook is based on the [official TensorFlow example of creating a DCGAN](https://www.tensorflow.org/tutorials/generative/dcgan). The code used here is inspired by the example and adapted to tune performance and work in Google Colab. Thus, note that the code used here originates from this example with modifications when needed.    \n",
    "\n",
    "**Before starting this notebook, create a folder at the root of your Google Drive named *CSI4106_Notebook9* (so the full filepath after adding this folder is /content/drive/My Drive/CSI4106_Notebook9). This directory will be used later in the notebook to save the trained models.**   \n",
    "\n",
    "**When submitting this notebook, ensure that you do NOT reset the outputs from running the code (plus remember to save the notebook with ctrl+s).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdkrlU777hBX"
   },
   "source": [
    "***HOMEWORK***:  \n",
    "Go through the notebook by running each cell, one at a time.  \n",
    "Look for **(TO DO)** for the tasks that you need to perform. Do not edit the code outside of the questions which you are asked to answer unless specifically asked. Once you're done, Sign the notebook (at the end of the notebook), and submit it.  \n",
    "\n",
    "*The notebook will be marked on 25.  \n",
    "Each **(TO DO)** has a number of points associated with it.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zwn2MjEV9fUc"
   },
   "source": [
    "**1.0 - Setting up the Google Colab Environment**   \n",
    "\n",
    "Before going straight into the code, we need to first set up our Google Colab environment. To do so we will need to install some libraries with pip (will need to do this each time we run through the notebook), we will need to import some libraries, and we will need to connect to our personal Google Drives (through your school account, which works even after migrating to Outlook, and we will finally need to enable GPU access for this notebook within Google Colab.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON0A993QGYx8"
   },
   "source": [
    "We will first initialize GPU access for the notebook. Google will ask that you turn it off when not in use, but you will be able to easily complete the notebook without issue. There are some limitations on how long it can be run for consecutively (many hours) and the total memory that can be used (around 12 GB depending on the allocated card), but these will not be issues for this notebook. Furthermore, Google may allocate different GPUs for each user. This may mean that you get allocated a slightly slower or faster card, but all GPUs will be more than sufficient to run these advanced models with ease. If a GPU-related issue occurs you will need to restart the notebook or disconnect/reconnect to a new GPU after a few minutes. However, this will be unlikely to occur during the notebook   \n",
    "\n",
    "**To connect to a GPU, go to *Edit* in the toolbar, select *Notebook settings*,  select *GPU* as the hardware accelerator, and click *Save*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIzX2_rlQ5nI"
   },
   "source": [
    "Next, run the following pip installs and import functions to set up TensorFlow and all other llibraries that we will use. Note that several libraries that we have used in previous notebooks return for solving this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sUKqfFLDcWJ"
   },
   "outputs": [],
   "source": [
    "# Install the specified libraries\n",
    "!pip install -q imageio\n",
    "!pip install -q git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkXguTw3-c6O"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow (note that this also will provide us with the MNIST dataset thanks to Keras)\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Install additional libraries to help define our arrays, images, and more\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "# This library allows us to connect to our Google Drive from Google Colab to read and write files to/from\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCAZ3def_B3Q"
   },
   "source": [
    "Next, you will need to mount your Google Drive to be the directory used by Google Colab. We cannot use local storage so all files used will need to be in your Google Drive. This is the default file location within the notebook, so we allow access to your Google Drive in the code below. When running the code below, you will be asked to give permission to access your drive. Even when submitting the notebook none of your information will be saved to the notebook (we will not be able to access your drive), so be sure to insert authorize access to the drive in order to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ae0MbkQ5-5Gr"
   },
   "outputs": [],
   "source": [
    "# Mount google drive containing the datasets\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rf49-SoMAI2U"
   },
   "source": [
    "Now that we have mounted to our Google Drive, we can access directories within the Drive. For example, since we have the *CSI4106_Notebook9* directory in the root of *My Drive*, we can access that filepath via the following directory:    \n",
    "- /content/drive/My Drive/CSI4106_Notebook9 (Do not click, just to show the filepath based on the folder that you have created)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiS37iPTJSTJ"
   },
   "source": [
    "**2.0 - Importing the Dataset**   \n",
    "\n",
    "Now that the environment is ready, we will load the MNIST dataset that is provided to us by the standard datasets within TensorFlow. The MNIST dataset is a dataset that consists of images of handwritten digits (1, 2, ..., 9). Below we load the data into our training sets (recall that we have no testing set since the GAN will be continuously training to improve itself, unlike the classification of the digits which would require a testing set). Then we look at the number of images that we will be using for training in the dataset and display and example image along with the digit that it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fetGkFPMKD_-"
   },
   "outputs": [],
   "source": [
    "# Load the handwritten digit images and their labels from the dataset\n",
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zx5uPPqvVmX"
   },
   "outputs": [],
   "source": [
    "# Show how many images are in the dataset\n",
    "print(\"There are\", len(train_images), \"many handwritten digits that will be used for training the DCGAN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XlP5DFDvyyB"
   },
   "outputs": [],
   "source": [
    "# Which digit is being shown\n",
    "print(\"The following image represents the handwritten digit\", train_labels[0])\n",
    "# What are its dimensions\n",
    "print(\"The image has the dimensions:\", train_images[0].shape, \"and is grayscale.\")\n",
    "# Display the digit\n",
    "plt.imshow(train_images[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZZoZUOi0htH"
   },
   "source": [
    "Below we can see the actual values for the grayscale image. As you can see, each pixel receives a single value between 0 and 255 to represent the grayscale to display for that pixel. We look at this since we will be normalizing these pixel values and modifying the structure of the image to be ready for use when training in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "54hQy9cL08a2"
   },
   "outputs": [],
   "source": [
    "# Understand the structure of the image and the pixel values used.\n",
    "train_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltL0M95VKWO_"
   },
   "source": [
    "Next, we will alter the received data to be ready to be used by the learning algorithms. We change the structure to add an additional dimension that simply states that the set of pixels represents a single digit. This helps since our models will need to work with batches of images and clearly know which pixels belong to which image.    \n",
    "\n",
    "The smaller an image's dimensions and the less values per pixel (i.e. 1 for grayscale and 3 for RGB) results in faster training (but a loss in data if the size is reduced). This image's size is fine (28 by 28), but we will normalize the pixel values from 0 to 255 to be within -1 and 1. Since Machine Learning algorithms learn from data, we typically want to normalize larger values from smaller values to ensure that the model learns the correct patterns and to minimize the cost of performing calculations with large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "unO-QyN5KMZh"
   },
   "outputs": [],
   "source": [
    "# Restructure the numpy array to display the pixel values as 28 (width) by 28 (height) by 1 (one image)\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmhT_JhF2GXK"
   },
   "source": [
    "Below we take a quick look at the pixel values for the first image of our modified training set (to view the normalization and new structure) along with a quick view of the image itself (which will look the exact same, but be plotted differently to match its new structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Afa_9yi32gpU"
   },
   "outputs": [],
   "source": [
    "# Which digit is being shown\n",
    "print(\"The following image represents the handwritten digit\", train_labels[0])\n",
    "# Look at the updated image (will look the same)\n",
    "plt.imshow(train_images[0][:, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M4bd2fz02YcN"
   },
   "outputs": [],
   "source": [
    "# Look at the normalized pixel values\n",
    "print(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wkjNbTQOosv"
   },
   "source": [
    "With the training data correctly processed we will now define the batch size to be used (how many samples will be trained at once) and we will define the full training set to be based on the sets of image batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GhglZQcOnTG"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000 # Define the amount to shuffle the images\n",
    "BATCH_SIZE = 256 # Define the batch size\n",
    "# Batch and shuffle the data with a random seed of 0\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE, seed=0).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycb-4MQnRMO1"
   },
   "source": [
    "**3.0 - Defining and Training the DCGAN**   \n",
    "\n",
    "With the dataset of images loaded into batches, we will now define our DCGAN. To do so we will define the various layers within the Neural Networks that are used by the Generator and Discriminator. Each of these models will require its own Neural Network since the two are competing with each other.      \n",
    "\n",
    "Below we define the Generator to be Sequential set of Convolutional Layers and activation functions (*Leaky ReLU is used here*) that take an input and outputs a 28 by 28 by 1 image. The values that are passed to the model are random values (referred to as *noise*) based on some probability distribution. The total number of values used as input is referred to as the *latent dimension* (in this example, it is 40). The goal of the Generator is to run the noise through its Neural Network to end up with an handwritten digit as an output.    \n",
    "\n",
    "The Generator defined below has four total layers. The input layer accepts the random noise as input (ouputting a larger number of outputs), performs *Batch Normalization* to help improve results, and runs the output through the *Leaky ReLU* activation function. This then goes into the first of three convolutional layers which each feed into each other after performing *Batch Normalization* and using the *Leaky ReLU* activation function. These each reduce the number of outputs to the final size of the image that we desire (28 by 28 by 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri4DZOt4PXwO"
   },
   "outputs": [],
   "source": [
    "def make_generator_model(batch_size):\n",
    "    '''\n",
    "    Defines a Generator to accept rnadom noise based on a probability distribution\n",
    "    and run it through a Neural Network of three convolutional layers with the Leaky ReLU\n",
    "    activiation function and Batch Normalization (which makes the learning process faster and more stable)\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    # Layer 1\n",
    "    model.add(layers.Dense(7*7*batch_size, use_bias=False, input_shape=(40,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7, 7, batch_size)))\n",
    "    assert model.output_shape == (None, 7, 7, batch_size) # Note: None is the batch size\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 4\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    # Return the Generator\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LijKVM7bHJtu"
   },
   "source": [
    "Below we create the generator that we will be using and look at an example output when passing random noise into the untrained model. As you can see, it currently has no representation of a handwritten digit as it has not learned the patterns from the digit images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-D9r9fscPpr-"
   },
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "generator = make_generator_model(BATCH_SIZE)\n",
    "# Generate a random noise input\n",
    "noise = tf.random.normal([1, 40])\n",
    "# Retrieve the outputted image from the Generator for the input\n",
    "generated_image = generator(noise, training=False)\n",
    "# Display the image generated by the Generator\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PP63JKGlJjuP"
   },
   "source": [
    "Next we define the Discriminator. The Discriminator accepts an image as input (with the expected dimensions) and runs it through a simple set of convolutional layers to output whether the image is real or fake. This model will be training against the Generator in an attempt to learn the attributes of fake and real handwritten digits. The Discriminator is defined to have two convolutional layers that are the mostly the same as what we have used in the Generator. Notice that the convolutional layers increase in size, rather than decrease in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_88c3ZzPq8G"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    '''\n",
    "    Create a Discriminator with two convolutional layers that accept a\n",
    "    handwritten digit image as input and outputs whether it is true or false.\n",
    "    Note that the Dropout code refers to the Dropout regularization technique that \n",
    "    can be used to enhance the performance of a Neural Network and achieve strong\n",
    "    results quicker than normal.\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXH0E3mMKM60"
   },
   "source": [
    "Below exhibits how what the untrained Discriminator outputs when we provide the image created by the Generator in the example above as input. Note that negative numbers mean that the image is predicted to be fake while positive numbers mean that the image is predicted to be real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Sw7zLZePsWP"
   },
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9A_3bfIWP837"
   },
   "source": [
    "With the models defined, we will now define the loss functions to be used when training and the optimizers to use. We will not go into detail regarding the code here, except for the parameter used for the Adam optimizer. However, comments have been added to explain the code at a high-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGRwZ6HnPx9f"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYWWrFtTP08W"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    '''\n",
    "    The loss function for the discriminator.\n",
    "    This must consider the combined loss from how well it performs at detecting fake and\n",
    "    real images.\n",
    "    '''\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qi171flMP3Z_"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    '''\n",
    "    The Generator loss is simply based on whether the generated image was able to \n",
    "    trick the discriminator in believing that the fake image is real.\n",
    "    '''\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWGgbZ02NRjQ"
   },
   "source": [
    "Below, the optimizers used to train the models are selected. We also used optimizers when working in the MLP notebook with scikit-learn. The main thing to note here is that the number being passed as input is the learning rate to be used by the models.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLncRgryP4Ve"
   },
   "outputs": [],
   "source": [
    "# Define the learning rate to be used by the optimizers\n",
    "lr = 1e-4\n",
    "# Set the optimizers that will be used by both models and set the learning rate.\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVlS9HsrOPv7"
   },
   "source": [
    "Next, this optional step is provided by the TensorFlow tutorial to showcase how we can save a model at various states during the training process. As we train the models through many epochs, this will allow us to save the state of a model and load it at any time. Note that the below assumes that you have set the directory in your drive as detailled in the Introduction of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM4j072ibATM"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/content/drive/My Drive/CSI4106_Notebook9'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beIKnJQ-OyPj"
   },
   "source": [
    "Similarly to the MLP notebook, we now determine how many epochs the model should be trained for. Although this will take several minutes to complete, 50 will be enough to see how the model's fake images evolve. The other parameters are the *latent dimension* that will be used and the number of examples that we will see for each epoch (to see how the Generator is improving over time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2kjCYcWP6hm"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 40\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim], seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxYwy7TZP2H-"
   },
   "source": [
    "Now we will define the training functions. The *train_step* function below accepts a batch of images from the training set as input and first collected a batch of fake images from the Generator. The Dirscriminator then attempts to determine, from the set of real images and the set of fake images, which images are real and which are fake. The loss from both models are then computed to find the gradients and backpropagate through the models to update the Generator and the Discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TTrNxuRQKQu"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images, batch_size):\n",
    "    '''\n",
    "    For a single batch of images from the training set, train the Discriminator\n",
    "    and Generator.\n",
    "    This function will automatically run on the default GPU of the system if it\n",
    "    can be detected by the environment.\n",
    "    '''\n",
    "    # Generate the batch of random noise inputs to be used to create the fake images\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      # Generate the fake images\n",
    "      generated_images = generator(noise, training=True)\n",
    "      # Have the Discriminator determine which images are real or fake from both sets of images\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # Calculate the loss for both the generator and the discriminator\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Compute the gradients for both the Generator and the Dsicriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # Update the models by applying the gradients to the models\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNMj1xb8SAs1"
   },
   "source": [
    "Similarly to what we used in the MLP notebook, we create a training function that trains each model with the entirety of the training set (in batches) for a certain number of epochs. After each epoch, a set of 16 example fake images are output to visualize the process. Checkpoints are also made into your specified Drive folder to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2eE_VSRQQAe"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    '''\n",
    "    Trains a Generator and Discriminator with a specified dataset for a specified\n",
    "    number of Epochs. Example outputs are saved to be visualized later.\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "      start = time.time()\n",
    "      # Loop through each batch in the training set\n",
    "      for image_batch in dataset:\n",
    "        # Train the models with the specified batch\n",
    "        train_step(image_batch, BATCH_SIZE)\n",
    "\n",
    "      # Produce images for the GIF as we go\n",
    "      display.clear_output(wait=True)\n",
    "      generate_and_save_images(generator,\n",
    "                              epoch + 1,\n",
    "                              seed)\n",
    "\n",
    "      # Save the model every 15 epochs\n",
    "      if (epoch + 1) % 15 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "      print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    # Generate after the final epoch\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                            epochs,\n",
    "                            seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FInn6iZFQR1t"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  '''\n",
    "  Generates fake images based on the Generator after being trained for a \n",
    "  specified number of epochs. We set the model to not train while generating the\n",
    "  images to ensure that only the training function will train the model.\n",
    "  '''\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KilxQSGwTtBd"
   },
   "source": [
    "Now we will train the DCGAN! Each epoch will take around 10-12 seconds, so you will see how it updates over the 50 epochs that we will run it for. In reality we may optimize the model more or run for more epochs, but this is enough to visualize the progress within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmX2U3O2QZHF"
   },
   "outputs": [],
   "source": [
    "# Remember to turn ON the GPU in Google Colab (the speed that it runs at will depend on the GPU that you are assigned;\n",
    "#                                              between 10 and 30 seconds on average)!!!\n",
    "# You are expected to run this through all of the epochs, not just a subset of them since everyone has the computation power for it.\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwe92a6hc1rr"
   },
   "source": [
    "With the training completed, we will load the checkpoint of our Generator to analyze the results from the training. After loading the checkpoint we will display some the collection of fake images that we displayed for epoch 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VuiWu6smQaM1"
   },
   "outputs": [],
   "source": [
    "# Load the checkpoint that is stored in your Google Drive folder\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-BxSYmLcHVE"
   },
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Olh_SRCcJeD"
   },
   "outputs": [],
   "source": [
    "# Display the output from the last epoch (epoch 50)\n",
    "plt.imshow(display_image(EPOCHS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCBNxQvxgkmv"
   },
   "source": [
    "**(TO DO) Q1**    \n",
    "a) To help you analyze the the evolution of the generated images while the Generator is learning, define the function *display_key_epochs* below. This function must display each collection of fake images that have been output for epochs *[10, 20, 30, 40, 50]*. After the function is defined, call it to display each each collection of fake images from the specified epochs. The code above this question shows how you can retrieve the fake outputs for a specific epoch.    \n",
    "b) Based on the observerd results from (a), did the model seem to perform well (i.e. do the generated digits look like handwritten digits)?        \n",
    "c) After visualizing the fake images through the epochs, what is happening to the generated outputs as it continues to be trained in each epoch? Explain why this process is happening.       \n",
    "d) During the last few epochs, how drastic are the changes being made to the fake images? Do they seem to be improving consistently at the same rate near the end of the training? Why or why not?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEzNGBKJYNq8"
   },
   "source": [
    "**(TO DO) Q1 (a) - 2 marks**    \n",
    "a) To help you analyze the the evolution of the generated images while the Generator is learning, define the function *display_key_epochs* below. This function must display each collection of fake images that have been output for epochs *\\[10, 20, 30, 40, 50\\]*. After the function is defined, call it to display each each collection of fake images from the specified epochs. The code above this question shows how you can retrieve the fake outputs for a specific epoch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_U2oDRnu7Ci5"
   },
   "outputs": [],
   "source": [
    "# TODO: Define the function for the target epochs and use it to display the images.\n",
    "# Note: This function is used later in the notebook and assumes that no input parameters will be added.\n",
    "def display_key_epochs():\n",
    "  ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyMj6i3KbRnm"
   },
   "source": [
    "**(TO DO) Q1 (b) - 1 mark**   \n",
    "b) Based on the observerd results from (a), did the model seem to perform well (i.e. do the generated digits look like handwritten digits)?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-B92DSLIbSrE"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOZTM9HtbTC4"
   },
   "source": [
    "**(TO DO) Q1 (c) - 2 marks**   \n",
    "c) After visualizing the fake images through the epochs, what is happening to the generated outputs as it continues to be trained in each epoch? Explain why this process is happening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmunYyTfbSH-"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e-dfpB5bR-W"
   },
   "source": [
    "**(TO DO) Q1 (d) - 2 marks**   \n",
    "d) During the last few epochs, how drastic are the changes being made to the fake images? Do they seem to be improving consistently at the same rate near the end of the training? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ueSO1QBbRP8"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLYSlV-akhL9"
   },
   "source": [
    "**4.0 - Testing Different Hyperparameters**   \n",
    "\n",
    "After seeing how the entire process is performed, you will be trying the same process out for yourself, but using different hyperparameters. By making minor changes to a small number of parameters, it is possible to obtain very different results. After you complete this process, you will discuss how the results compare with the test that is provided to you in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYzn2vU5lL9G"
   },
   "source": [
    "**(TO DO) Q2 - 6 marks**    \n",
    "You will now create a new Generator and Discriminator such that they will work with a *batch size* of 64 images per batch and will set the optimizer to use a larger *learning rate* of 1e-3. This will not require any updates to the defined structures of the Generator and Discriminator and will simply involve copying some code over and making minor modifications. Below is a list of each task that you will need to perform. Each of these should be done in the corresponding code cell below. The structure is provided for you, so you simply must fill in the blanks.    \n",
    "\n",
    "1) Update the batch size variable to specify that the batches should consist of 64 images and redefine *train_dataset* to use the new batch size.    \n",
    "2) Redefine the Generator object with the new batch size as input to the function.    \n",
    "3) Redefine the Discriminator object.\n",
    "4) Redefine the *cross_entropy* variable the same way it was previously.    \n",
    "5) Set the learning rate to 1e-3 and re-define the optimizer variables.     \n",
    "6) Run the provided *train_step* function to recompile the code after making the above changes.    \n",
    "7) Train the new models for 50 epochs with the dataset defined in 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBwigInSdkVZ"
   },
   "outputs": [],
   "source": [
    "# TODO: 1) Update the batch size variable to specify that the batches should consist of 64 images and redefine train_dataset to use the new batch size. \n",
    "# Update the batch size\n",
    "BATCH_SIZE = ...\n",
    "# Update train_dataset\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9ELACgoqmru"
   },
   "outputs": [],
   "source": [
    "# TODO: 2) Redefine the Generator object with the new batch size as input to the function. \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0wPjNKpqpU-"
   },
   "outputs": [],
   "source": [
    "# TODO: 3) Redefine the Discriminator object.\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BL7ZReOprGbs"
   },
   "outputs": [],
   "source": [
    "# TODO: 4) Redefine the cross_entropy variable the same way it was previously\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MouOPyjq0qX"
   },
   "outputs": [],
   "source": [
    "# TODO: 5) Set the learning rate to 1e-3 and re-define the optimizer variables\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zMniHxORyb1K"
   },
   "outputs": [],
   "source": [
    "# (6) Re-compile this function to work with the updates (just run this code cell)\n",
    "@tf.function\n",
    "def train_step(images, batch_size):\n",
    "    '''\n",
    "    For a single batch of images from the training set, train the Discriminator\n",
    "    and Generator.\n",
    "    This function will automatically run on the default GPU of the system if it\n",
    "    can be detected by the environment.\n",
    "    '''\n",
    "    # Generate the batch of random noise inputs to be used to create the fake images\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      # Generate the fake images\n",
    "      generated_images = generator(noise, training=True)\n",
    "      # Have the Discriminator determine which images are real or fake from both sets of images\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # Calculate the loss for both the generator and the discriminator\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Compute the gradients for both the Generator and the Dsicriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # Update the models by applying the gradients to the models\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hi0Lfs_Arl5Y"
   },
   "outputs": [],
   "source": [
    "# TODO: 7) Train the new models for 50 epochs with the dataset defined in 1)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqT_1YKnzt2N"
   },
   "source": [
    "**(TO DO) Q3**   \n",
    "Now that you have modified the hyperparameters and trained the new DCGAN, you will discuss how this model compares to the model from part 3.0 of this notebook and some observations based on the results.    \n",
    "a) Load the latest checkpoint from your Google Drive folder (the same way seen before) and use your *display_key_epochs* function to display the results from the key epochs to use for the rest of the question.     \n",
    "b) Between the model that you defined and trained and the example model from part 3.0, which performed better and why do you think it performed better?   \n",
    "c) Name one pro and one con from increasing the learning rate (based on your observations and/or based on what you know the learning rate does).      \n",
    "d) Name one pro and one con from decreasing the batch size (based on your observations and/or based on what you know happens when the batch size is decreased)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KR10X8CUaVpy"
   },
   "source": [
    "**(TO DO) Q3 (a) - 1 mark**   \n",
    "a) Load the latest checkpoint from your Google Drive folder (the same way seen before) and use your *display_key_epochs* function to display the results from the key epochs to use for the rest of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4y9z_XSaWAL"
   },
   "outputs": [],
   "source": [
    "# TODO: Restore the latest checkpoint and display the key epochs\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4-cFq8eKuqk"
   },
   "source": [
    "**(TO DO) Q3 (b) - 1 mark**   \n",
    "b) Between the model that you defined and trained and the example model from part 3.0, which performed better and why do you think it performed better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4SLbIAKvaT"
   },
   "source": [
    "TO DO ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaQmjP3VKv5R"
   },
   "source": [
    "**(TO DO) Q3 (c) - 2 marks**   \n",
    "c) Name one pro and one con from increasing the learning rate (based on your observations and/or based on what you know the learning rate does).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbAX7-GzKwBB"
   },
   "source": [
    "TO DO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APej4biXKwQB"
   },
   "source": [
    "**(TO DO) Q3 (d) - 2 marks**   \n",
    "d) Name one pro and one con from decreasing the batch size (based on your observations and/or based on what you know happens when the batch size is decreased)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzHyl9tiKwWL"
   },
   "source": [
    "TO DO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47uiDNuT0gOp"
   },
   "source": [
    "**5.0 - Testing a Simpler DCGAN**   \n",
    "Now that you have explored a basic DCGAN and have played around with the learning rate and batch size hyperparameters, we will go through one final test of an even simpler DCGAN.    \n",
    "\n",
    "In this scenario, everything will be the same as your setup in section 4.0 of this notebook, except that we will redefine the DCGAN to remove one of the convolutional layers from the Generator and the Discriminator. This will result in a more simplistic model. By comparing the results obtained here to the results obtained in the previous section, you will discuss whether an increased complexity or decreased complexity helps the model generate better fake handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02Z90B-ONMEy"
   },
   "source": [
    "The Generator below now is setup to be the same as before, but now contains only three layers, with two convolutional layers. Specifically, the largest convolutional layer has been removed which results in the input layer's output leading to a smaller convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AhLrM2og2Bxa"
   },
   "outputs": [],
   "source": [
    "def make_generator_model(batch_size):\n",
    "    '''\n",
    "    Defines a Generator to accept rnadom noise based on a probability distribution\n",
    "    and run it through a Neural Network of two convolutional layers with the Leaky ReLU\n",
    "    activiation function and Batch Normalization (which makes the learning process faster and more stable)\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    # Layer 1\n",
    "    model.add(layers.Dense(7*7*batch_size, use_bias=False, input_shape=(40,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((7, 7, batch_size)))\n",
    "    assert model.output_shape == (None, 7, 7, batch_size) # Note: None is the batch size\n",
    "\n",
    "    # Layer 2\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    # Layer 3\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    # Return the Generator\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3RPOs1F2Z4G"
   },
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "generator = make_generator_model(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQ8LN7t5NlDu"
   },
   "source": [
    "Similarly, the Discriminator removed one of its two convolutional layers. This results in only a single convolutional layer to learn how to distinguish which images are real and which are fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EteEkB0D2IHW"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    '''\n",
    "    Create a Discriminator with one convolutional layer that accept a\n",
    "    handwritten digit image as input and outputs whether it is true or false.\n",
    "    Note that the Dropout code refers to the Dropout regularization technique that \n",
    "    can be used to enhance the performance of a Neural Network and achieve strong\n",
    "    results quicker than normal.\n",
    "    '''\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sA_Q-w432cVl"
   },
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mg9w8Zwb2hkx"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Set the optimizers that will be used by both models and set the learning rate.\n",
    "generator_optimizer = tf.keras.optimizers.Adam(lr)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLNl4MyP2sYT"
   },
   "outputs": [],
   "source": [
    "# Re-compile this function to work with the updates\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images, batch_size):\n",
    "    '''\n",
    "    For a single batch of images from the training set, train the Discriminator\n",
    "    and Generator.\n",
    "    This function will automatically run on the default GPU of the system if it\n",
    "    can be detected by the environment.\n",
    "    '''\n",
    "    # Generate the batch of random noise inputs to be used to create the fake images\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      # Generate the fake images\n",
    "      generated_images = generator(noise, training=True)\n",
    "      # Have the Discriminator determine which images are real or fake from both sets of images\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "      # Calculate the loss for both the generator and the discriminator\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    # Compute the gradients for both the Generator and the Dsicriminator\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    # Update the models by applying the gradients to the models\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHBceA6Y2u4b"
   },
   "outputs": [],
   "source": [
    "# Remember to turn ON the GPU in Google Colab.\n",
    "# You are expected to run this through all of the epochs, not just a subset of them since everyone has the computation power for it.\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Si8tMA-N16fJ"
   },
   "source": [
    "**(TO DO) Q4**          \n",
    "a) Load the latest checkpoint from your Google Drive folder (the same way seen before) and use your *display_key_epochs* function to display the results from the key epochs to use for the rest of the question.       \n",
    "b) How did removing one of the convolutional layers from the Generator and the Discriminator affect the training?    \n",
    "c) How did removing one of the convolutional layers from the Generator and the Discriminator affect the results when compared to the model from part 4.0? Also, how did removing this affect the results seen from the outputs?  \n",
    "d) If the images that we use were larger in size, would a complex model or a simple model be better to use in this scenario? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msn1rXz_bv2d"
   },
   "source": [
    "**(TO DO) Q4 (a) - 1 mark**     \n",
    "a) Load the latest checkpoint from your Google Drive folder (the same way seen before) and use your *display_key_epochs* function to display the results from the key epochs to use for the rest of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYvVLq5TbwOU"
   },
   "outputs": [],
   "source": [
    "# TODO ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCigdxeRO6P5"
   },
   "source": [
    "**(TO DO) Q4 (b) - 1 mark**    \n",
    "b) How did removing one of the convolutional layers from the Generator and the Discriminator affect the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrryS1I5PO9k"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPzN2uHhPPQb"
   },
   "source": [
    "**(TO DO) Q4 (c) - 2 marks**    \n",
    "(c) How did removing one of the convolutional layers from the Generator and the Discriminator affect the results when compared to the model from part 4.0? Also, how did removing this affect the results seen from the outputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0-CbolVPPUY"
   },
   "source": [
    "TODO ...     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsJIauTEPPC4"
   },
   "source": [
    "**(TO DO) Q4 (d) - 2 marks**    \n",
    "d) If the images that we use were larger in size, would a complex model or a simple model be better to use in this scenario? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmrsUQzXPOtl"
   },
   "source": [
    "TODO ...    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QLJJ7CFhnq7"
   },
   "source": [
    "***SIGNATURE:***\n",
    "My name is --------------------------.\n",
    "My student number is -----------------.\n",
    "I certify being the author of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gk5bNGV2hoOR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CSI4106-GAN_Fall20.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
